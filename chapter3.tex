\section{Introduction}

In Chapters 1 and 2, I presented a model of perceptual choice and showed it can systematically predict the repulsion effect, but not the attraction effect. In this chapter, I test another prediction of the model while demonstrating an important empirical result in another domain: best-worst choice.

\subsection{Introducing Best-Worst Choice}

Best-worst choice is an experimental paradigm where participants select their most and least preferred options from a choice set. Originally proposed by \textcite{finn1992determining}, best-worst choice is widely used in a number of applied fields, such as transportation \parencite{beck2016best} and healthcare economics \parencite{cheung2016using,flynn2007best,muhlbacher2016experimental}. One key advantage, when compared to traditional discrete choice research, is that researchers can use best-worst choices to gain information about participants' ranking of options while never requiring them to complete a full ranking task \parencite{marleyProbabilisticModelsBest2005}.

Researchers have developed theoretical models to account for best-worst choice data. Most best-worst choice models relate best-worst choices to an underlying utility function. \textcite{marleyProbabilisticModelsBest2005} developed a class of models known as the \textit{maxdiff} (maximum difference) models of best-worst choice\footnote{Note that the term maxdiff is sometimes erroneously used to refer to best-worst experiments in the generic sense. Following \textcite{marleyProbabilisticModelsBest2005}, I use maxdiff to refer to a specific class and parameterization of this choice model.}. According to the maxdiff model, given choice set $K$, the probability of selecting option $x$ as best and option $y$ as worst, where $x \neq y$, is defined computed as:

\begin{equation}
   BW_{K}(x,y)=\frac{e^{u_{x}-u_{y}}}{\sum_{\substack{{p,q}\in K\\p \neq q}} e^{u_{p}-u_{q}}}   
   \label{eqn:maxdiff_equation}
\end{equation}

where $u_{i}$ is the utility of option $i$. This model proposes a single utility scale  for selection of both the best option and the worst option in a choice set. It assumes that best-choice probabilities are an increasing function of $u$, while worst-choice probabilities are a decreasing function of $u$. The use of the exponential function means that the maxdiff model is another form of the widely used multinomial logit (MNL) choice model \parencite{hausman1984specification}. Utilities are also assumed to be independent.

There are several variants of this model along with other best-worst choice models \parencite{marleyProbabilisticModelsBest2005,marleyProbabilisticModelsSetdependent2008,marleyModelsBestWorst2012,flynnBestWorstScaling2007,flynn2014best}, though the maxdiff model from Equation~\ref{eqn:maxdiff_equation} remains the dominant model for analyzing best-worst choice data.

The maxdiff model predicts a monotonic relationship between best-choice probabilities and worst-choice probabilities \parencite{hawkinsBestTimesWorst2014}. Researchers have explored whether this monotonicity holds empirically. \textcite{hawkinsIntegratingCognitiveProcess2014a} examined both preferential and perceptual best-worst choice data using response time modeling. They used the linear ballistic accumulator model (LBA) \parencite{brownSimplestCompleteModel2008b}, which casts the decision process as a race between "accumulators" towards a threshold, where the average accumulation across trials is captured by the drift rate parameter. Modeling datasets containing both preferential and perceptual best-worst choice data, they were able to successfully account for choice data by assuming a parallel race between "best" and "worst" accumulators for each option. Furthermore, they showed that the utility values estimated for each option using a MNL model were positively linearly related to the log drift rate values from the LBA, suggesting an underlying utility representation that explains both types of choices. 

In a follow-up article, \textcite{hawkinsBestTimesWorst2014} found that, collapsing across choice sets, best-choice probabilities are monotonically related to worst-choice probabilities. Options that were most likely to be selected as best were least likely to be selected as worst, and vice versa. This finding held for perceptual choice and consumer choice. They also showed that, using the parallel best-worst LBA as a model, the drift rate parameter for worst choice can be parameterized as the reciprocal of the best choice drift rate. Formally, if $d_{b}(i)$ is the drift rate for selecting option $i$ as best, then $d_{w}(i)=1/d_{b}(i)$, where $d_{w}(i)$ is option $i$'s drift rate for worst choices. 

Both the parallel best-worst LBA and the maxdiff model assume that the utilities of all options presented are independent. However, as I show below, the perceptual model from Chapters 1 and 2 predicts, under certain conditions, a dissociation between best and worst choices inconsistent with this assumption.

\subsection{Model-Based Dissociations in Best-Worst Choice}

Here, I use the perceptual model to make a prediction regarding a dissociation between best choices and worst choices. 

Let $K$ be a choice set consisting of options $T$, $C$, and $D$ (i.e., target, competitor, and decoy). As in Experiments 1 and 2, the options are rectangles in a perceptual choice experiment. As in Chapter 2, I assume that on each trial $i$ with choice set $K$, the perceived area $\mathbf{X_{i}}$ of all 3 stimuli is sampled from a multivariate Gaussian distribution with a mean vector $\boldsymbol{\mu}$ and variance-covariance matrix $\boldsymbol{\Sigma}$ (see Equation~\ref{eqn:mvnorm}).

$\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ are parameterized the same as in Chapters 1 and 2. 

Following conventions in the literature, I use $B(j)$ to denote the probability of selecting option $j$ as best and $W(k)$ to denote the probability of selecting option $k$ as worst. I also require the model (and participants in the experiment) to never select the same option as best and as worst.

I assume that, given a vector $\mathbf{X_{i}}$ of perceived areas on trial $i$ with set $K$, the probability a participant selects stimulus $j$ as best is:

\begin{equation}
   B(j|i,K)=P(\mathbf{X}_{ij}>\mathbf{X}_{ik}), \forall k \in K, j \neq k
   \label{eqn:bchoice1}
\end{equation}

while the probability of selecting stimulus $j$ as worst is:

\begin{equation}
   W(j|i,K)=P(\mathbf{X}_{ij}<\mathbf{X}_{ik}), \forall k \in K, j \neq k
   \label{eqn:wchoice1}
\end{equation}

Simply put, the option with the largest perceived area is always selected as best, while the option with the smallest perceived area is always selected as worst. 

As it happens, the correlations (i.e., $\Omega$) estimated from Experiment 2 predict that, in a best-worst choice paradigm, best and worst-choice probabilities are non-monotonically related. 

I computed predictions for best-worst choice by simulating the model using the mean parameters ($\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$) estimated from Experiment 2\footnote{I used only those estimated from the triangle condition (See Experiment 2).} and simulated a large number of trials ($N=1,000,000$). As in previous simulations, I collapsed over choice set (i.e., whether the target is wide or tall) and focused on target, competitor, and decoy choice proportions at each level of TDD. I present these results in Figure~\ref{fig:bw_sim}, in a series of state-trace plots \parencite{bamber1979state,ashby2022state}. In a state-trace plot, the analyst plots two dependent variables against one another in each experimental condition. 

\begin{figure}
   \includegraphics[width=\linewidth]{figures/bw_preds_sigma_constant_comp_effect_no_outliers.jpeg}
   \caption{Simulated best-worst predictions per the Thurstonian perceptual model. Each row is a different TDD value from Experiment 2.}
   \label{fig:bw_sim}
\end{figure}

The model, conditioned on the estimated parameters, predicts an interesting result. Although the competitor is most frequently chosen as best, due to the repulsion effect from Experiment 2 and from \textcite{spektorWhenGoodLooks2018b} Experiment 3, it is not, however, least frequently chosen as worst. Specifically, $B(C)>B(T)$, while $W(T)<W(C)$. At lower levels of $TDD$, the model even predicts that competitor and decoy are chosen at similar rates. As we will see, this prediction does not bear out empirically, and this prediction is likely due to the fact that participants are less sensitive to perceptual differences when providing ratings than when making choices \parencite{gronau2023choice}.

The Thurstonian perceptual model predicts this because $\rho_{TD}>\rho_{CD}\approx\rho_{TC}$. On the (relatively few) trials where $X_{iD}$ is largest, it is more likely that $X_{iD}>X_{iT}>X_{iC}$ than $X_{iD}>X_{iC}>X_{iT}$. In other words, the high $\rho_{TD}$ value "pulls up" the target more than the competitor. The similarity, and comparability, of target and decoy entail that the repulsion effect at the best-choice level (Experiment 2) does not necessarily show up at the worst-choice level.

The maxdiff model predicts a monotonic state-trace plot. That is, if we assume all options' utilities are independent, the model predicts that best-choice probabilities are negatively related to worst-choice probabilities. To demonstrate this, I simulated the maxdiff model using randomly generated independent utilities. See Figure~\ref{fig:maxdiff:sim}. 

\begin{figure}
   \includegraphics[width=\linewidth]{figures/maxdiff_sim_monotonic.jpeg}
   \caption{Best-worst choice simulations using the maxdiff model. Each curve is a separate simulation. The model predicts a negative, monotonic relationship between best-choice and worst-choice.}
   \label{fig:maxdiff_sim}
\end{figure}

This effect is subtle, and the predicted effect size is small. Indeed, all differences in predicted $W(C)-W(T)$ values were $<.05$. In Experiment 3, I show the empirical and modeling results from a best-worst choice experiment designed to test this prediction. I show that the dissociation between best and worst choices does indeed occur. I also show that the assumption of monotonicity required by the maxdiff model can fail empirically.

\section{Experiment 3}

The goal of Experiment 3 was to test the predictions of the Thurstonian perceptual choice model. Specifically, the perceptual model predicts that $B(C)>B(T)$ but $W(T)<W(C)$. To test this prediction, I used stimuli identical to those of Experiment 2 and presented stimuli in the triangle display of Experiments 1 and 2.
I show that 1) this prediction holds empirically and 2) the maxdiff model cannot account for these results, even when all parameters are free to vary. 

\subsection{Methods}

\subsubsection{Participants.}
Data collection took place at the University of Massachusetts Amherst. $392$ undergraduate students participated in exchange for course credit. $23$ participants who achieved less than $80\%$ accuracy on catch trials (see below) were excluded from all analyses. Trials with response times (RTs) $<100\text{ms}$ or  $>10000\text{ms}$ were also excluded from all analyses.

\subsubsection{Stimuli.}
The experiment had three types of trials: critical trials, filler trials, and catch trials. 

Stimuli on critical trials were identical to those of Experiment 2. On each critical trial, the target and competitor had the same area but differed on orientation, with one stimulus being wide and the other tall. The decoy always had the same orientation as the target. I varied TDD at $2\%$, $5\%$, $9\%$, and $14\%$. I also varied the target, competitor, and decoy rectangles along three diagonals, as in Experiment 2. 

On each filler trial, three stimuli were uniformly sampled from the space between the largest and smallest diagonals.

On each catch trial, one stimulus was sampled from the largest diagonal, while two stimuli were sampled from the smallest diagonal.

\subsubsection{Design.}
There were 8 blocks of trials. In each block there were 24 critical trials, 6 at each TDD level. There were 8 trials per diagonal. In addition to the critical trials, there were 10 filler trials and 3 catch trials per block.

Participants were randomly assigned into one of two conditions: best-worst or worst-best. On each trial, participants in the best-worst condition initially chose the largest rectangle and then chose the smallest rectangle. Participants in the worst-best condition chose in the opposite order. The condition factor was included to account for the possibility that best-worst choice order impacts choice. Results were consistent regardless of condition, so I collapsed over this factor in the analyses reported below.

After removing participants, there were 185 participants in the best-worst condition and 184 participants in the worst-best condition.

Stimuli were presented on computer monitors with a resolution of 1920 x 1080 pixels. The experiment was programmed with GNU Octave and Psychtoolbox \parencite{octave,brainardPsychophysicsToolbox1997}. 

\subsubsection{Procedure.}

The experiment began with three practice trials, which were identical to the filler trials. 

On each trial, participants saw three rectangles, labeled 1, 2, and 3 (from left to right), arranged in a triangle display. Participants in the best-worst (worst-best) condition saw a prompt asking them to select the largest (smallest) rectangle on screen. Participants used the mouse to click on their chosen rectangle. After they made their choice, this rectangle changed color to indicate that it was no longer available as an option. Next, participants in the best-worst (worst-best) condition selected the smallest (largest) rectangle, at which point the trial ended. See Figure~\ref{fig:bw_example_trial} for an example trial.

\begin{figure}
   \includegraphics[width=\linewidth]{figures/bw_design_fig.jpg}
   \caption{A sample experimental trial from Experiment 3. Note that this is a trial in the best-worst condition.}
   \label{fig:bw_example_trial}
 \end{figure}
 
Stimulus order was randomized on each trial. 

Participants were told their percentage correct of best choices, worst choices, and overall choices at the end of the experiment.

\subsection{Results}


\subsubsection{Catch Trials.}
Participants performed well on the catch trials. The mean percentage correct for best choices was $97.97\% (SD=14.09)$, and the mean percentage correct for worst choices was $98.26\% (SD=13.09)$. The mean percentage correct for both best and worst choices (i.e., the mean percentage of the trials on which participants were able to correctly identify the largest and smallest rectangles) was $96.98\% (SD=17.12)$. 

\subsubsection{Filler Trials.}
Participants performed worse on the filler trials compared to the catch trials, but still well above chance. The mean percentage correct for best choices was $89.83\% (SD=30.23)$, and the mean percentage correct for worst choices was $88.95\% (SD=13.09)$. The mean percentage correct for both best and worst choices was $96.98\% (SD=17.12)$. 

\subsubsection{Critical Trials.}

First, I computed the mean choice proportions for each distinct rectangle, collapsed across choice set. Here, I replicated the findings of \textcite{hawkinsBestTimesWorst2014}, that, when ignoring the effect of context, best choices and worst choices are monotonically related. These data are plotted in Figure~\ref{fig:bw_marginal}.

\begin{figure}
   \includegraphics[width=\linewidth]{figures/crit_mean_props_marginal.jpeg}
   \caption{Experiment 3 marginal mean best and worst-choice proportions for all unique rectangles, collapsed across choice set. X and Y axis error bars are $95\%$ CIs.}
   \label{fig:bw_marginal}
\end{figure}

Next, I analyzed choice proportions by conditioning on TDD and choice set. Mean choice proportions for these data are plotted in Figure~\ref{fig:bw_mean_choice_by_set}. 

\begin{figure}
   \includegraphics[width=100mm]{figures/crit_mean_choice_by_set_dist_labelHW.jpeg}
   \caption{Experiment 3 mean best and worst-choice proportions for the $h$, $w$, and $d$ rectangles, conditioned on TDD (rows) and choice set (shapes).}
   \label{fig:bw_mean_choice_by_set}
\end{figure}

Participants showed a consistent bias to choose $w$ (the wider rectangle) as largest, a finding also shown in Experiments 1 and 2. Participants also (on average) regularly choose the decoy rectangle as smallest, with the exception of the choice set $h,w,d_{w}$ and $TDD=2\%$, where they selected the $h$ rectangle as smallest, on average. This can be attributed to the difficulty of the $TDD=2\%$ condition and the overall wide rectangle bias. However, consistent with the predictions of the model, the target was still less likely to be chosen as worst than the competitor, $W(h|{h,w,d_{h}})<W(h|{h,w,d_{w}})$ and $W(w|{h,w,d_{w}})<W(w|{h,w,d_{h}})$, while the competitor option was more likely to be chosen as best, $B(h|{h,w,d_{w}})>B(h|{h,w,d_{h}})$ and $B(w|{h,w,d_{h}})>B(w|{h,w,d_{w}})$. 

These results are more easily understood by plotting mean target, competitor, and decoy choice proportions across TDD levels, collapsed over choice set. See Figure~\ref{fig:bw_mean_choice_collapsed} for these data. 

The best-choice proportions replicated the repulsion effect initially found by \textcite{spektorWhenGoodLooks2018b} and replicated in Experiment 2, where the competitor was more likely to be chosen as best at low TDD levels, while the target and competitor were chosen equally often at high TDD levels. Decoy best-choice proportions also decrease systematically with TDD. See the Appendix for inferential statistics which support these conclusions.

Furthermore, the target is always more likely to be chosen as worst, compared to the competitor and decoy, at all TDD levels, $W(T)<W(C)$, as predicted by the perceptual model outlined in Chapter 2. This model still cannot predict the null best-choice repulsion effect when $TDD=14\%$, as discussed in Chapter 2, which suggests that this effect may be due to higher level decision processes.

\begin{figure}
   \includegraphics[width=100mm]{figures/crit_mean_props_by_dist.jpeg}
   \caption{Experiment 3 mean best and worst-choice proportions for the target, competitor and decoy rectangles, conditioned on TDD (rows).}
   \label{fig:bw_mean_choice_collapsed}
\end{figure}

\subsubsection{Maxdiff Modeling}

I first turn to the maxdiff model \parencite{marleyProbabilisticModelsBest2005}, which was outlined in the introduction to this chapter. This equation predicts that the probability of choosing options $x$ as best and $y$ as worst, $x \neq y$, increases monotonically with the difference in their estimated utilities (see Equation~\ref{eqn:maxdiff_equation}). This model is the most commonly used analysis technique for best-worst choice data. I applied this model to the current experiment and show that it is unable to predict the observed dissocations in best-worst choices, even with its best fitting parameters.

I implemented this model as a Bayesian hierarchical model. I show the details of the model fitting procedure, including parameterization, parameter estimates, and all priors in the Appendix and focus on the model predictions in the main text. The model predictions for the mean best and worst choices are shown in Figure~\ref{fig:maxdiff_collapsed_preds}.

The model clearly mispredicts the data. The model predicts a monotonic relationship between best choices and worst choices. It predicts a repulsion effect in both best choices and worst choices, i.e., $B(C)>B(T)>B(D)$ and $W(D)>W(T)>W(C)$. Given that, in the data, $B(C)-B(T)>W(C)-W(T)$, the best-fitting parameter set is the one that predicts a repulsion effect. 

The target-competitor misprediction stems from the fact that the model choices come from the utility of each option, calculated through a linear combination of experimental factors and model coefficients, including target/competitor/decoy status. The model could, if the data suggest it, predict that the target has greater utility than the competitor or vice versa. However, because best-choice proportions are positively related to utility and worst-choice proportions are negatively related to utility, the model cannot simultaneously predict $B(C)>B(T)$ and $W(T)<W(C)$. 

I also show participant-level data and model predictions in Figure~\ref{fig:maxdiff_sub_preds}. The model generally does a poor job at accounting for participant worst-choice proportions.
\begin{figure}
   \includegraphics[width=\linewidth]{figures/maxdiff_1_means_model_v_data.jpeg}
   \caption{Experiment 3 maxdiff model predictions for the mean target, competitor, and decoy best-worst choice proportions.}
   \label{fig:maxdiff_collapsed_preds}
\end{figure}

\begin{figure}
   \includegraphics[width=\linewidth]{figures/maxdiff_1_subjectmeans_model_v_data.jpeg}
   \caption{Experiment 3 maxdiff model predictions for the mean target, competitor, and decoy best-worst participant-level choice proportions, conditioned on TDD (rows) and choice type, i.e. best v. worst (columns). Vertical error bars are $95\%$ HDIs.}
   \label{fig:maxdiff_sub_preds}
\end{figure}

\section{Discussion}
In Experiment 3, I showed that, in support of the Thurstonian perceptual choice model introduced in Chapter 2, correlated valuations can induce dissociations in best-worst choices. Specifically, given a target, competitor, and decoy option (borrowing the terminology of the attraction effect), the competitor is more likely than the target to be selected as best ($B(C)>B(T)$), but the competitor is also more likely than the target to be selected as worst ($W(C)>W(T)$). This prediction was made using the perceptual choice model of Chapter 2, conditioned on the parameters estimated from Experiment 3. Furthermore, the prediction was made with different set of participants and a completely different experimental task. This level of predictive success is atypical in psychology and even relatively uncommon in the cognitive modeling literature. 

The maxdiff model, the most common analysis technique for best-worst choice \parencite{marleyProbabilisticModelsBest2005,hawkinsIntegratingCognitiveProcess2014a,muhlbacher2016experimental,de2017relations}, cannot accomodate these results.  

The maxdiff model assumes that, when selecting the best and worst option, the decision-maker picks the option with the highest and lowest utility, respectively. The utilities of each option are independently distributed, and the decision-maker uses the same utility for both best and worst choices. The use of a single utility scale is to some extent a convenience assumption. \textcite{marleyProbabilisticModelsBest2005} explored several theoretical models, including the case where best and worst utilities exist on independent ratio scales, though such a model does not seem to have been adopted by substantive researchers. \textcite{marleyProbabilisticModelsSetdependent2008} demonstrated set-dependent best-worst choice models, which allows for different context dependence based on choice sets, albeit with best and worst choices still a function on a common underlying utility scale. 

\textcite{hawkins2019like} also argued that best and worst choices rely on a common utility representation. They fit the maxdiff model to 5 best-worst datasets and showed, via Bayesian mixture modeling, that the overwhelming majority of participants were best fit by a model with a single utility representation. 

\textcite{gervzinivc2021estimating} argued (and provided evidence for) the claim that while best and worst choices rely on a common utility scale, people use two distinct decision rules for best choices and worst choices. They argued that the former is compensatory (i.e., allowing tradeoffs between attributes), while the latter is non-compensatory (i.e., disallowing tradeoffs to minimize future regret). 

The Thurstonian perceptual model, used for the current predictions, also employs a common utility scale for both best and worst choices. However, the model does not assume independently distributed utilities, as assumed by the maxdiff model \parencite{de2017relations}. Thus, the claims of \textcite{hawkinsBestTimesWorst2014} and \textcite{hawkins2019like} are not necessarily falsified; rather, they are amended to account for correlations between option utilities.

Due to the small effect size, I required a large amount of data to estimate these dissociations. Most best-worst choice research is applied (for example in transportation and healthcare economics), where researchers do not typically have access a large amount of participant-level data. Thus, researchers are unlikely to observe the dissociations in best-worst choice and will analyze the data using the maxdiff model. They may then arrive at incorrect conclusions regarding participants' preferences. 

% In a related paradigm, researchers have shown that allowing participants to reject all options from a set can affect choice. \textcite{tverskyChoiceConflictDynamics1992a} showed that people are more likely to reject all options if the options trade off on attributes compared to if one dominates the other. Other researchers have demonstrated that forced choice and free choice (when rejection is possible) can create markedly different choice patterns \parencite{dhar1997consumer,dhar1997context,dhar1996effect,dharEffectForcedChoice2003b,noguchiDescriptionexperienceGapChoice2016a,brazellNochoiceOptionDual2006b,parker2011rejectable,chernevChoiceOverloadConceptual2015}. 

I did not fit the Thurstonian perceptual model to Experiment 3. In Experiment 2, I asked participants estimate the size of the stimuli and used these direct size ratings to estimate the model parameters. It also seems unreasonable to expect researchers to fit a multivariate Thurstonian model to most best-worst choice studies, given limitations in data and potential issues with parameter identifiability. 

The central purpose for conducting best-worst choice studies is to identify participants' preference distributions on a set of options. Best-worst choice is less cognitively demanding on participants than asking them to rank all options and far more efficient than pairwise forced choices on all combinations of options \parencite{louviere2008modeling}. In many cases, analyzing best-worst data with the maxdiff model may be the best approach, especially if researchers have no reason to believe that options are strongly correlated. It is an open question, left for future research, whether correlations between options in applied choice research can create similar dissociations in best-worst choice. 

The current study only considered Case 3 best-worst choice \parencite{marleyModelsBestWorst2012}, where the attributes of options (in our case, height/width, TDD, diagonal) are systematically manipulated to examine their impact on preferences. I ignored Case 1 best-worst choice, where researchers are interested in preference for each option as a whole (e.g., a consumer's preference for cars over bicycles) or Case 2 best-worst choice, where researchers ask participants to select their preferred attribute from a set (e.g., a consumer's preference for short waiting times over easily accessible WiFi in a clinic). Future research should consider ways to generalize the current paradigm and results to the other best-worst choice types.

For the time being, I have identified a discrepancy between theory and data, in a prominent area of decision-making research. This gap is both theoretically and practically interesting. It is up to future researchers, myself included, to continue theoretical development in this line of study. 